import os
import sys
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from keras.datasets import mnist
from keras.models import Sequential, Model, save_model, load_model
from keras.layers import Input, Conv2D, Reshape, Flatten, Dense, Dropout, LeakyReLU
from keras.layers import BatchNormalization, Activation, UpSampling2D
from keras.optimizers import Adam

class GAN:
    def __init__(self, rows, cols):
        self.img_rows = rows        #can be used to change the shape of the image, as may be the case with different datasets
        self.img_cols = cols
        self.channels = 1
        self.img_shape = (self.img_rows, self.img_cols, self.channels)
        self.latent_dim = 100                                               
        #dimension of the latent variable space 
        #the latent variable space can have any arbitary number of dimensions
        #as it is just noise and will be mapped to generator images by the generator
        
        optimizer = Adam(lr=2e-4)

        self.discriminator = self.build_discriminator()
        self.discriminator.compile(optimizer = optimizer, loss = 'binary_crossentropy',metrics = ['accuracy'])

        self.generator = self.build_generator()

        z = Input(shape=(self.latent_dim, )) #random noise input
        img = self.generator(z)   #image generated by the generator

        self.discriminator.trainable = False 

        validity = self.discriminator(img)    #checking whether image is real or fake

        self.combined = Model(inputs=z, outputs=validity)   #combining the generator and discriminator
        self.combined.compile(loss = 'binary_crossentropy',
                              optimizer = optimizer)

    def build_generator(self):

        model = Sequential()
        model.add(Dense(256, input_dim = self.latent_dim))
        model.add(LeakyReLU(alpha=0.2))
        model.add(BatchNormalization())
        model.add(Dense(512))
        model.add(LeakyReLU(alpha=0.2))
        model.add(BatchNormalization())
        model.add(Dense(1024))
        model.add(LeakyReLU(alpha=0.2))
        model.add(BatchNormalization())
        model.add(Dense(self.img_rows*self.img_cols*self.channels, activation="tanh"))
        model.add(Reshape(self.img_shape))   #reshaping output of the MLP into image

        model.summary()

        noise = Input(shape=(self.latent_dim, ))
        img = model(noise)

        return Model(inputs=[noise], outputs=[img])

    def build_discriminator(self):

            model = Sequential()
            model.add(Flatten(input_shape=self.img_shape))
            model.add(Dense(512))
            model.add(LeakyReLU(alpha=0.2))
            model.add(Dense(256))
            model.add(LeakyReLU(alpha=0.2))
            model.add(Dense(1, activation="sigmoid"))   #outputs validity of image

            model.summary()

            img = Input(shape=self.img_shape)
            validity = model(img)

            return Model(inputs = [img], outputs = [validity])

    def train(self, epochs, batch_size = 128, sample_interval = 50, k=1):
            
            (x_train,_), (_,_) = mnist.load_data()   #loading mnist data

            x_train = x_train/127.5 - 1             #rescaling 0-255 to -1 to 1
            x_train = np.expand_dims(x_train, axis = 3)

            valid = np.ones((batch_size, 1))
            fake = np.zeros((batch_size, 1))

            for epoch in range(epochs):
                
                for i in range(k):   #you can train the discriminator multiple times before moving on to the training of the generator although here I have set k = 1
                    indices = np.random.randint(0, x_train.shape[0], batch_size)
                    imgs = x_train[indices]

                    noise = np.random.normal(0, 1, (batch_size, self.latent_dim))

                    generated_imgs = self.generator.predict(noise)
                
                    self.discriminator.trainable = True
                    d_loss_real = self.discriminator.train_on_batch(imgs, valid)
                    d_loss_fake = self.discriminator.train_on_batch(generated_imgs, fake)
                    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)
                #here the disriminator has been trained only once in each epoch
                #howvever, it can be trained multiple times and then we can train the generator
                #though training only once is the least computationally expensive option

                noise = np.random.normal(0, 1, (batch_size, self.latent_dim))  #creating noise from a normal distribution whose mean is 0 and standard deviation is 1

                self.discriminator.trainable = False
                g_loss = self.combined.train_on_batch(noise, valid) #train the generator so that discriminator outputs valid(1) on the image produced by the generator from the noise

                print("Epoch %d [[D loss: %f, acc %.2f%%]] [[G loss: %f]]" % (epoch, d_loss[0], 100*d_loss[1], g_loss ))
                
                if epoch % sample_interval == 0:
                    self.sample_images(epoch) 
                    self.save_model(epoch)     

    def sample_images(self, epoch):
                r = 5
                c = 5
                noise = np.random.normal(0, 1, (r*c, self.latent_dim))  
                gen_imgs = self.generator.predict(noise)

                gen_imgs = 0.5 * gen_imgs + 0.5

                fig, ax = plt.subplots(r, c)
                counter = 0
                for i in range(r):
                    for j in range(c):
                        ax[i,j].imshow(gen_imgs[counter, :, :, 0], cmap='gray')
                        ax[i,j].axis('off')
                        counter+=1

                fig.savefig("./saved_images/images"+str(epoch)+".png")
                plt.close()

    def save_model(self, epoch):
                self.generator.save("./saved_models-gans/generator"+str(epoch)+".h5")
                self.discriminator.save("./saved_models-gans/discriminator"+str(epoch)+".h5")

if __name__ == '__main__':
    gan = GAN(28, 28)
    gan.train(epochs=80000, batch_size = 32, sample_interval = 200) #trained for 80000 epochs sampling images at intervals of 200

